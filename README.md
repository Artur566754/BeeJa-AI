# BeeJa AI Model ü§ñ

–°–æ–±—Å—Ç–≤–µ–Ω–Ω–∞—è –ò–ò –º–æ–¥–µ–ª—å —Å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å—é –æ–±—É—á–µ–Ω–∏—è –Ω–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏—Ö –¥–∞—Ç–∞—Å–µ—Ç–∞—Ö, –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–º –¥–æ–æ–±—É—á–µ–Ω–∏–µ–º –∏ —á–∞—Ç-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–æ–º. –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –∫–∞–∫ LSTM, —Ç–∞–∫ –∏ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—É—é Transformer –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É (–∞–Ω–∞–ª–æ–≥ GPT).

## –í–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏

- üß† **–î–≤–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –º–æ–¥–µ–ª–µ–π**:
  - **LSTM** - –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–∞—è —Ä–µ–∫—É—Ä—Ä–µ–Ω—Ç–Ω–∞—è —Å–µ—Ç—å (–ø–æ—Å–∏–º–≤–æ–ª—å–Ω–∞—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è)
  - **Transformer** - —Å–æ–≤—Ä–µ–º–µ–Ω–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ —Å multi-head attention (BPE —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è, –∫–∞–∫ –≤ GPT)
- üìö **–ú—É–ª—å—Ç–∏—Ñ–æ—Ä–º–∞—Ç–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö** - –ø–æ–¥–¥–µ—Ä–∂–∫–∞ .txt, .pdf, .docx —Ñ–∞–π–ª–æ–≤
- üîÑ **–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –¥–æ–æ–±—É—á–µ–Ω–∏–µ** - –º–æ–¥–µ–ª—å –æ–±—É—á–∞–µ—Ç—Å—è –ø—Ä–∏ –¥–æ–±–∞–≤–ª–µ–Ω–∏–∏ –Ω–æ–≤—ã—Ö —Ñ–∞–π–ª–æ–≤
- üí¨ **–ß–∞—Ç-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å** - –æ–±—â–∞–π—Ç–µ—Å—å —Å –º–æ–¥–µ–ª—å—é —á–µ—Ä–µ–∑ –∫–æ–º–∞–Ω–¥–Ω—É—é —Å—Ç—Ä–æ–∫—É
- üéØ **–ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã** - Transformer –∑–∞–ø–æ–º–∏–Ω–∞–µ—Ç –∏—Å—Ç–æ—Ä–∏—é —Ä–∞–∑–≥–æ–≤–æ—Ä–∞ (–¥–æ 5 —Å–æ–æ–±—â–µ–Ω–∏–π)
- üíæ **–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤–µ—Å–æ–≤** - –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç —Å–≤–æ–∏ –∑–Ω–∞–Ω–∏—è –º–µ–∂–¥—É —Å–µ—Å—Å–∏—è–º–∏
- üõ°Ô∏è **–ó–∞—â–∏—Ç–∞ –æ—Ç –æ—à–∏–±–æ–∫** - –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –æ—Ç–∫–∞—Ç –ø—Ä–∏ –Ω–µ—É–¥–∞—á–Ω–æ–º –æ–±—É—á–µ–Ω–∏–∏
- ‚ö° **–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏** - –ø–æ–¥–¥–µ—Ä–∂–∫–∞ GPU –∏ mixed precision training
- üìä **–ü—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä—ã** - –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ—Ü–µ—Å—Å–∞ –æ–±—É—á–µ–Ω–∏—è —Å tqdm

## –ß—Ç–æ –Ω–æ–≤–æ–≥–æ –≤ Transformer –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ?

### –†–∞–Ω—å—à–µ (LSTM):
- –ü–æ—Å–∏–º–≤–æ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ (–∫–∞–∂–¥–∞—è –±—É–∫–≤–∞ = —Ç–æ–∫–µ–Ω)
- –ö–æ—Ä–æ—Ç–∫–∞—è –ø–∞–º—è—Ç—å, –∑–∞–±—ã–≤–∞–µ—Ç –¥–ª–∏–Ω–Ω—ã–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç—ã
- –ü—Ä–æ—Å—Ç–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞
- –°–ª–æ–≤–∞—Ä—å ~100-200 —Å–∏–º–≤–æ–ª–æ–≤

### –°–µ–π—á–∞—Å (Transformer):
- **BPE —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è** - —É–º–Ω—ã–µ –ø–æ–¥—Å–ª–æ–≤–∞ (5000 —Ç–æ–∫–µ–Ω–æ–≤ –≤–º–µ—Å—Ç–æ —Å–∏–º–≤–æ–ª–æ–≤)
  - –ü—Ä–∏–º–µ—Ä: "–ø—Ä–∏–≤–µ—Ç" = 1-2 —Ç–æ–∫–µ–Ω–∞ –≤–º–µ—Å—Ç–æ 6 –±—É–∫–≤
  - –ú–æ–¥–µ–ª—å —É—á–∏—Ç—Å—è –±—ã—Å—Ç—Ä–µ–µ –∏ –ø–æ–Ω–∏–º–∞–µ—Ç –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—é —Å–ª–æ–≤
- **Multi-Head Attention** - –º–æ–¥–µ–ª—å "—Å–º–æ—Ç—Ä–∏—Ç" –Ω–∞ —Ç–µ–∫—Å—Ç —Å 4-16 —Ä–∞–∑–Ω—ã—Ö —Ç–æ—á–µ–∫ –∑—Ä–µ–Ω–∏—è –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ
- **–ö–æ–Ω—Ç–µ–∫—Å—Ç –¥–æ 256-1024 —Ç–æ–∫–µ–Ω–æ–≤** - –ø–æ–Ω–∏–º–∞–µ—Ç –¥–ª–∏–Ω–Ω—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏
- **–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∏—Å—Ç–æ—Ä–∏–µ–π –¥–∏–∞–ª–æ–≥–∞** - –∑–∞–ø–æ–º–∏–Ω–∞–µ—Ç –ø—Ä–µ–¥—ã–¥—É—â–∏–µ —Å–æ–æ–±—â–µ–Ω–∏—è
- **–ü—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è** - greedy, top-k, nucleus sampling, temperature control

## –£—Å—Ç–∞–Ω–æ–≤–∫–∞

### –¢—Ä–µ–±–æ–≤–∞–Ω–∏—è

- Python 3.8+
- pip

### –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π

```bash
pip install -r requirements.txt
```

## –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ

### –í—ã–±–æ—Ä –º–æ–¥–µ–ª–∏

–°–∏—Å—Ç–µ–º–∞ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –¥–≤–∞ —Ç–∏–ø–∞ –º–æ–¥–µ–ª–µ–π:

1. **LSTM** - –±—ã—Å—Ç—Ä–∞—è, –ø—Ä–æ—Å—Ç–∞—è, –ø–æ—Å–∏–º–≤–æ–ª—å–Ω–∞—è (–¥–ª—è —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤)
2. **Transformer** - —Å–æ–≤—Ä–µ–º–µ–Ω–Ω–∞—è, –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–∞—è, —Å BPE —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–µ–π (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è)

### 1. –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏

#### –ë—ã—Å—Ç—Ä—ã–π —Å—Ç–∞—Ä—Ç (Transformer Small)

```bash
# –û–±—É—á–µ–Ω–∏–µ –Ω–∞ –≤—Å–µ—Ö –¥–∞—Ç–∞—Å–µ—Ç–∞—Ö –∏–∑ –ø–∞–ø–∫–∏ datasets/
python main.py --train --model-type transformer --model-size small --epochs 50 --batch-size 64 --lr 0.0005
```

#### LSTM –º–æ–¥–µ–ª—å (–¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è)

```bash
python main.py --train --model-type lstm --epochs 100 --lr 0.001
```

#### –†–∞–∑–º–µ—Ä—ã Transformer –º–æ–¥–µ–ª–∏

- `small` - **—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –¥–ª—è –Ω–∞—á–∞–ª–∞**
  - 4 —Å–ª–æ—è, 128 dim, 4 –≥–æ–ª–æ–≤—ã –≤–Ω–∏–º–∞–Ω–∏—è
  - –ë—ã—Å—Ç—Ä–æ–µ –æ–±—É—á–µ–Ω–∏–µ, —Ö–æ—Ä–æ—à–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ
  - –ü–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è CPU –∏ GPU
  
- `medium` - –±–∞–ª–∞–Ω—Å –∫–∞—á–µ—Å—Ç–≤–∞ –∏ —Å–∫–æ—Ä–æ—Å—Ç–∏
  - 6 —Å–ª–æ–µ–≤, 256 dim, 8 –≥–æ–ª–æ–≤ –≤–Ω–∏–º–∞–Ω–∏—è
  - –õ—É—á—à–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ, —Ç—Ä–µ–±—É–µ—Ç –±–æ–ª—å—à–µ –≤—Ä–µ–º–µ–Ω–∏
  - –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è GPU
  
- `large` - –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ
  - 12 —Å–ª–æ–µ–≤, 512 dim, 16 –≥–æ–ª–æ–≤ –≤–Ω–∏–º–∞–Ω–∏—è
  - –õ—É—á—à–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ, –¥–æ–ª–≥–æ–µ –æ–±—É—á–µ–Ω–∏–µ
  - –¢—Ä–µ–±—É–µ—Ç GPU —Å –±–æ–ª—å—à–æ–π –ø–∞–º—è—Ç—å—é

#### –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è

- `--epochs` - –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è: 50-500 –¥–ª—è Transformer, 100+ –¥–ª—è LSTM)
- `--lr` - —Å–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è:
  - `0.0005` - –º–µ–¥–ª–µ–Ω–Ω–æ, –Ω–æ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –¥–ª—è Transformer)
  - `0.0001` - –æ—á–µ–Ω—å –º–µ–¥–ª–µ–Ω–Ω–æ, –¥–ª—è –¥–æ–æ–±—É—á–µ–Ω–∏—è
  - `0.001` - —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ –¥–ª—è LSTM
- `--batch-size` - —Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞:
  - `64` - —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –¥–ª—è Transformer –Ω–∞ GPU
  - `32` - –¥–ª—è CPU –∏–ª–∏ –º–∞–ª–æ–π –ø–∞–º—è—Ç–∏
  - `8-16` - –¥–ª—è large –º–æ–¥–µ–ª–∏
- `--mixed-precision` - —É—Å–∫–æ—Ä–µ–Ω–∏–µ –Ω–∞ GPU (—Ç–æ–ª—å–∫–æ –¥–ª—è CUDA)

#### –ü—Ä–∏–º–µ—Ä—ã –∫–æ–º–∞–Ω–¥

```bash
# –ö–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ Transformer (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è)
python main.py --train --model-type transformer --model-size small --epochs 500 --batch-size 64 --lr 0.0005

# –ë—ã—Å—Ç—Ä–æ–µ –æ–±—É—á–µ–Ω–∏–µ –¥–ª—è —Ç–µ—Å—Ç–æ–≤
python main.py --train --model-type transformer --model-size small --epochs 30 --batch-size 32 --lr 0.001

# Transformer medium —Å GPU –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π
python main.py --train --model-type transformer --model-size medium --epochs 100 --batch-size 32 --lr 0.0003 --mixed-precision

# Transformer large (—Ç—Ä–µ–±—É–µ—Ç –º–æ—â–Ω—ã–π GPU)
python main.py --train --model-type transformer --model-size large --epochs 200 --batch-size 16 --lr 0.0001 --mixed-precision

# LSTM –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
python main.py --train --model-type lstm --epochs 100 --lr 0.001 --batch-size 32
```

#### –ß—Ç–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è?

1. **–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤** - –≤—Å–µ .txt —Ñ–∞–π–ª—ã –∏–∑ –ø–∞–ø–∫–∏ `datasets/`
2. **–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ BPE —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞** - —Å–æ–∑–¥–∞–Ω–∏–µ —Å–ª–æ–≤–∞—Ä—è –∏–∑ 5000 —Ç–æ–∫–µ–Ω–æ–≤ (–∑–∞–Ω–∏–º–∞–µ—Ç 2-5 –º–∏–Ω—É—Ç)
3. **–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö** - —Å–æ–∑–¥–∞–Ω–∏–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
4. **–û–±—É—á–µ–Ω–∏–µ —Å –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–∞–º–∏**:
   ```
   ============================================================
   Starting training: 500 epochs, 45231 sequences
   Batch size: 64, Learning rate: 0.0005
   ============================================================
   
   Training Progress:  15%|‚ñà‚ñà‚ñà‚ñà‚ñå         | 75/500 [12:30<1:11:20, 10.07s/epoch]
   Epoch 75/500: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 707/707 [00:10<00:00, 68.5batch/s] loss: 2.1234, avg_loss: 2.1456
   ```
   
   –í—ã —É–≤–∏–¥–∏—Ç–µ:
   - –û–±—â–∏–π –ø—Ä–æ–≥—Ä–µ—Å—Å –ø–æ —ç–ø–æ—Ö–∞–º
   - –ü—Ä–æ–≥—Ä–µ—Å—Å –±–∞—Ç—á–µ–π –≤–Ω—É—Ç—Ä–∏ –∫–∞–∂–¥–æ–π —ç–ø–æ—Ö–∏
   - Loss (–ø–æ—Ç–µ—Ä–∏) - –¥–æ–ª–∂–Ω—ã —É–º–µ–Ω—å—à–∞—Ç—å—Å—è
   - Perplexity (—Å–ª–æ–∂–Ω–æ—Å—Ç—å) - –¥–æ–ª–∂–Ω–∞ —É–º–µ–Ω—å—à–∞—Ç—å—Å—è
   - –í—Ä–µ–º—è –Ω–∞ —ç–ø–æ—Ö—É –∏ –æ—Ü–µ–Ω–∫—É –æ—Å—Ç–∞–≤—à–µ–≥–æ—Å—è –≤—Ä–µ–º–µ–Ω–∏

### 2. –ß–∞—Ç —Å –º–æ–¥–µ–ª—å—é

#### Transformer –º–æ–¥–µ–ª—å (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è)

```bash
python main.py --chat --model-type transformer --model-size small
```

–∏–ª–∏ –∫–æ—Ä–æ—á–µ:

```bash
python main.py --model-type transformer --model-size small
```

#### LSTM –º–æ–¥–µ–ª—å

```bash
python main.py --chat --model-type lstm
```

–∏–ª–∏ –ø—Ä–æ—Å—Ç–æ:

```bash
python main.py
```

#### –û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏ —á–∞—Ç–∞ —Å Transformer

- –ó–∞–ø–æ–º–∏–Ω–∞–µ—Ç –ø–æ—Å–ª–µ–¥–Ω–∏–µ 5 —Å–æ–æ–±—â–µ–Ω–∏–π
- –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –±–æ–ª–µ–µ —Å–≤—è–∑–Ω—ã–µ –∏ –æ—Å–º—ã—Å–ª–µ–Ω–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã
- –ü–æ–Ω–∏–º–∞–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç —Ä–∞–∑–≥–æ–≤–æ—Ä–∞
- –ò—Å–ø–æ–ª—å–∑—É–µ—Ç nucleus sampling –¥–ª—è —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è –æ—Ç–≤–µ—Ç–æ–≤

#### –ö–æ–º–∞–Ω–¥—ã –≤ —á–∞—Ç–µ

- `/exit` - –≤—ã—Ö–æ–¥ –∏–∑ —á–∞—Ç–∞
- `/retrain` - –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–∏
- `/clear` - –æ—á–∏—Å—Ç–∏—Ç—å –∏—Å—Ç–æ—Ä–∏—é —Ä–∞–∑–≥–æ–≤–æ—Ä–∞ (—Ç–æ–ª—å–∫–æ Transformer)

### 3. –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–±—É—á–µ–Ω–∏–µ

–í–∫–ª—é—á–∏—Ç–µ —Ä–µ–∂–∏–º –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è:

```bash
python main.py --auto --model-type transformer --model-size small
```

–ú–æ–¥–µ–ª—å –±—É–¥–µ—Ç –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø–µ—Ä–µ–æ–±—É—á–∞—Ç—å—Å—è –ø—Ä–∏ –¥–æ–±–∞–≤–ª–µ–Ω–∏–∏ –Ω–æ–≤—ã—Ö —Ñ–∞–π–ª–æ–≤ –≤ –ø–∞–ø–∫—É `datasets/`.

## –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –ø—Ä–æ–µ–∫—Ç–∞

```
.
‚îú‚îÄ‚îÄ src/                    # –ò—Å—Ö–æ–¥–Ω—ã–π –∫–æ–¥
‚îÇ   ‚îú‚îÄ‚îÄ model.py           # LSTM –º–æ–¥–µ–ª—å
‚îÇ   ‚îú‚îÄ‚îÄ transformer_model.py  # Transformer –º–æ–¥–µ–ª—å
‚îÇ   ‚îú‚îÄ‚îÄ transformer_components.py  # Multi-head attention, FFN, etc.
‚îÇ   ‚îú‚îÄ‚îÄ tokenizer.py       # BPE —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä (5000 —Ç–æ–∫–µ–Ω–æ–≤)
‚îÇ   ‚îú‚îÄ‚îÄ text_generator.py  # –°—Ç—Ä–∞—Ç–µ–≥–∏–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ (greedy, top-k, nucleus)
‚îÇ   ‚îú‚îÄ‚îÄ conversation_context.py  # –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∏—Å—Ç–æ—Ä–∏–µ–π –¥–∏–∞–ª–æ–≥–∞
‚îÇ   ‚îú‚îÄ‚îÄ evaluation.py      # –ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–µ–π
‚îÇ   ‚îú‚îÄ‚îÄ dataset_loader.py  # –ó–∞–≥—Ä—É–∑—á–∏–∫ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤
‚îÇ   ‚îú‚îÄ‚îÄ training_pipeline.py  # –ü–∞–π–ø–ª–∞–π–Ω –æ–±—É—á–µ–Ω–∏—è —Å –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–∞–º–∏
‚îÇ   ‚îú‚îÄ‚îÄ chat_interface.py  # –ß–∞—Ç-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å
‚îÇ   ‚îú‚îÄ‚îÄ vocabulary.py      # –°–ª–æ–≤–∞—Ä—å —Å–∏–º–≤–æ–ª–æ–≤ (LSTM)
‚îÇ   ‚îî‚îÄ‚îÄ config.py          # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
‚îú‚îÄ‚îÄ datasets/              # –î–∞—Ç–∞—Å–µ—Ç—ã –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
‚îÇ   ‚îú‚îÄ‚îÄ all_combined.txt  # –û–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç
‚îÇ   ‚îî‚îÄ‚îÄ *.txt             # –í–∞—à–∏ –¥–∞—Ç–∞—Å–µ—Ç—ã
‚îú‚îÄ‚îÄ models/                # –°–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏
‚îÇ   ‚îú‚îÄ‚îÄ ai_model.pth      # LSTM checkpoint
‚îÇ   ‚îú‚îÄ‚îÄ transformer_small.pth   # Transformer small
‚îÇ   ‚îú‚îÄ‚îÄ transformer_medium.pth  # Transformer medium
‚îÇ   ‚îî‚îÄ‚îÄ transformer_large.pth   # Transformer large
‚îú‚îÄ‚îÄ tests/                 # –¢–µ—Å—Ç—ã (54+ property-based —Ç–µ—Å—Ç–æ–≤)
‚îÇ   ‚îú‚îÄ‚îÄ unit/             # Unit —Ç–µ—Å—Ç—ã
‚îÇ   ‚îú‚îÄ‚îÄ property/         # Property-based —Ç–µ—Å—Ç—ã (Hypothesis)
‚îÇ   ‚îî‚îÄ‚îÄ integration/      # –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏–æ–Ω–Ω—ã–µ —Ç–µ—Å—Ç—ã
‚îú‚îÄ‚îÄ telegram_bot/          # Telegram –±–æ—Ç –¥–ª—è –º–æ–¥–µ–ª–∏
‚îú‚îÄ‚îÄ main.py               # –ì–ª–∞–≤–Ω—ã–π —Ñ–∞–π–ª –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è
‚îú‚îÄ‚îÄ combine_datasets.py   # –°–∫—Ä–∏–ø—Ç –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è –¥–∞—Ç–∞—Å–µ—Ç–æ–≤
‚îî‚îÄ‚îÄ requirements.txt      # –ó–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏
```

## –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤

### –°–ø–æ—Å–æ–± 1: –û—Ç–¥–µ–ª—å–Ω—ã–µ —Ñ–∞–π–ª—ã

1. –ü–æ–º–µ—Å—Ç–∏—Ç–µ —Ñ–∞–π–ª—ã –≤ –ø–∞–ø–∫—É `datasets/`
2. –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã:
   - `.txt` - —Ç–µ–∫—Å—Ç–æ–≤—ã–µ —Ñ–∞–π–ª—ã (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è)
   - `.pdf` - PDF –¥–æ–∫—É–º–µ–Ω—Ç—ã
   - `.docx` - Word –¥–æ–∫—É–º–µ–Ω—Ç—ã
3. –ó–∞–ø—É—Å—Ç–∏—Ç–µ –æ–±—É—á–µ–Ω–∏–µ: `python main.py --train --model-type transformer --model-size small`

### –°–ø–æ—Å–æ–± 2: –û–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è)

–î–ª—è —É–¥–æ–±—Å—Ç–≤–∞ –º–æ–∂–Ω–æ –æ–±—ä–µ–¥–∏–Ω–∏—Ç—å –≤—Å–µ –¥–∞—Ç–∞—Å–µ—Ç—ã –≤ –æ–¥–∏–Ω —Ñ–∞–π–ª:

```bash
python combine_datasets.py
```

–≠—Ç–æ —Å–æ–∑–¥–∞—Å—Ç —Ñ–∞–π–ª `datasets/all_combined.txt` —Å–æ –≤—Å–µ–º–∏ –≤–∞—à–∏–º–∏ –¥–∞–Ω–Ω—ã–º–∏.

### –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –¥–∞—Ç–∞—Å–µ—Ç–∞–º

- **–ú–∏–Ω–∏–º—É–º**: 10KB —Ç–µ–∫—Å—Ç–∞ –¥–ª—è Transformer, 1KB –¥–ª—è LSTM
- **–û–ø—Ç–∏–º–∞–ª—å–Ω–æ**: 100KB+ –¥–ª—è —Ö–æ—Ä–æ—à–µ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞
- **–§–æ—Ä–º–∞—Ç**: –æ–±—ã—á–Ω—ã–π —Ç–µ–∫—Å—Ç, UTF-8 –∫–æ–¥–∏—Ä–æ–≤–∫–∞
- **–Ø–∑—ã–∫**: –ª—é–±–æ–π (—Ä—É—Å—Å–∫–∏–π, –∞–Ω–≥–ª–∏–π—Å–∫–∏–π, —Å–º–µ—à–∞–Ω–Ω—ã–π)

## –ù–∞—á–∞–ª—å–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç

–ú–æ–¥–µ–ª—å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞ —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ —Å–æ–∑–¥–∞—Ç–µ–ª—è—Ö:
- **–°–æ–∑–¥–∞—Ç–µ–ª—å**: Jamsaide (–ê—Ä—Ç—É—Ä)
- **–ü–æ–º–æ—â–Ω–∏–∫–∏**: BeeBoo (–ù–∏–∫–∏—Ç–∞) –∏ Araxium (–õ–µ–≤)

## –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –º–æ–¥–µ–ª–µ–π

### LSTM –º–æ–¥–µ–ª—å (—Å—Ç–∞—Ä–∞—è)

- **–¢–∏–ø**: Character-level LSTM
- **Embedding —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å**: 128
- **Hidden —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å**: 256
- **–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–µ–≤**: 2
- **–¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è**: –ü–æ—Å–∏–º–≤–æ–ª—å–Ω–∞—è (~100-200 —Å–∏–º–≤–æ–ª–æ–≤)
- **–û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä**: Adam
- **Loss —Ñ—É–Ω–∫—Ü–∏—è**: CrossEntropyLoss
- **–ö–æ–Ω—Ç–µ–∫—Å—Ç**: –û–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–π (–∑–∞–±—ã–≤–∞–µ—Ç –¥–ª–∏–Ω–Ω—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏)

### Transformer –º–æ–¥–µ–ª—å (–Ω–æ–≤–∞—è, —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è)

#### Small (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –¥–ª—è –Ω–∞—á–∞–ª–∞) ‚≠ê

- **Embedding —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å**: 128
- **–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≥–æ–ª–æ–≤ –≤–Ω–∏–º–∞–Ω–∏—è**: 4
- **–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–µ–≤**: 4
- **FFN —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å**: 512
- **–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞**: 256 —Ç–æ–∫–µ–Ω–æ–≤
- **–¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è**: BPE (vocab_size=5000)
- **Dropout**: 0.1
- **–ü–∞—Ä–∞–º–µ—Ç—Ä—ã**: ~2M

#### Medium (–±–∞–ª–∞–Ω—Å –∫–∞—á–µ—Å—Ç–≤–∞ –∏ —Å–∫–æ—Ä–æ—Å—Ç–∏)

- **Embedding —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å**: 256
- **–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≥–æ–ª–æ–≤ –≤–Ω–∏–º–∞–Ω–∏—è**: 8
- **–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–µ–≤**: 6
- **FFN —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å**: 1024
- **–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞**: 512 —Ç–æ–∫–µ–Ω–æ–≤
- **–¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è**: BPE (vocab_size=5000)
- **Dropout**: 0.1
- **–ü–∞—Ä–∞–º–µ—Ç—Ä—ã**: ~15M

#### Large (–º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ)

- **Embedding —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å**: 512
- **–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≥–æ–ª–æ–≤ –≤–Ω–∏–º–∞–Ω–∏—è**: 16
- **–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–µ–≤**: 12
- **FFN —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å**: 2048
- **–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞**: 1024 —Ç–æ–∫–µ–Ω–æ–≤
- **–¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è**: BPE (vocab_size=5000)
- **Dropout**: 0.1
- **–ü–∞—Ä–∞–º–µ—Ç—Ä—ã**: ~100M

### –û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏ Transformer

- **Multi-Head Self-Attention**: –ú–æ–¥–µ–ª—å "—Å–º–æ—Ç—Ä–∏—Ç" –Ω–∞ —Ç–µ–∫—Å—Ç —Å –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Ç–æ—á–µ–∫ –∑—Ä–µ–Ω–∏—è –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ
- **Positional Encoding**: –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø–æ–∑–∏—Ü–∏–∏ —Ç–æ–∫–µ–Ω–æ–≤ (sin/cos —Ñ—É–Ω–∫—Ü–∏–∏)
- **Causal Masking**: –ü—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ—Ç "–ø–æ–¥–≥–ª—è–¥—ã–≤–∞–Ω–∏–µ" –≤ –±—É–¥—É—â–µ–µ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏
- **Layer Normalization**: –°—Ç–∞–±–∏–ª–∏–∑–∏—Ä—É–µ—Ç –æ–±—É—á–µ–Ω–∏–µ
- **Residual Connections**: –£–ª—É—á—à–∞–µ—Ç –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π –ø–æ—Ç–æ–∫ —á–µ—Ä–µ–∑ –≥–ª—É–±–æ–∫–∏–µ —Å–ª–æ–∏
- **Memory-Efficient Attention**: –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –¥–ª—è –¥–ª–∏–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π (>512 —Ç–æ–∫–µ–Ω–æ–≤)
- **Gradient Clipping**: –ü—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ—Ç –≤–∑—Ä—ã–≤–∞—é—â–∏–µ—Å—è –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã

### BPE –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è

–í–º–µ—Å—Ç–æ –ø–æ—Å–∏–º–≤–æ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏, Transformer –∏—Å–ø–æ–ª—å–∑—É–µ—Ç BPE (Byte Pair Encoding):

**–ü—Ä–∏–º–µ—Ä:**
- –°—Ç–∞—Ä—ã–π —Å–ø–æ—Å–æ–± (LSTM): "–ø—Ä–∏–≤–µ—Ç" = [–ø, —Ä, –∏, –≤, –µ, —Ç] (6 —Ç–æ–∫–µ–Ω–æ–≤)
- –ù–æ–≤—ã–π —Å–ø–æ—Å–æ–± (Transformer): "–ø—Ä–∏–≤–µ—Ç" = [–ø—Ä–∏, –≤–µ—Ç] (2 —Ç–æ–∫–µ–Ω–∞)

**–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:**
- –ú–æ–¥–µ–ª—å —É—á–∏—Ç—Å—è –±—ã—Å—Ç—Ä–µ–µ
- –ü–æ–Ω–∏–º–∞–µ—Ç –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—é –∏ —Å—Ç—Ä—É–∫—Ç—É—Ä—É —Å–ª–æ–≤
- –ú–µ–Ω—å—à–µ —Ç–æ–∫–µ–Ω–æ–≤ = –±–æ–ª—å—à–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
- –°–ª–æ–≤–∞—Ä—å 5000 —Ç–æ–∫–µ–Ω–æ–≤ –ø–æ–∫—Ä—ã–≤–∞–µ—Ç –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–æ —Å–ª–æ–≤ –∏ –∏—Ö —á–∞—Å—Ç–µ–π

### –°—Ç—Ä–∞—Ç–µ–≥–∏–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞

Transformer –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ —Å—Ç—Ä–∞—Ç–µ–≥–∏–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏:

1. **Greedy Decoding** - –≤—ã–±–æ—Ä –Ω–∞–∏–±–æ–ª–µ–µ –≤–µ—Ä–æ—è—Ç–Ω–æ–≥–æ —Ç–æ–∫–µ–Ω–∞ (–¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ)
2. **Top-K Sampling** - –≤—ã–±–æ—Ä –∏–∑ K –Ω–∞–∏–±–æ–ª–µ–µ –≤–µ—Ä–æ—è—Ç–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤ (k=50)
3. **Nucleus (Top-P) Sampling** - –≤—ã–±–æ—Ä –∏–∑ —Ç–æ–∫–µ–Ω–æ–≤ —Å —Å—É–º–º–∞—Ä–Ω–æ–π –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å—é P (p=0.9)
4. **Temperature Scaling** - –∫–æ–Ω—Ç—Ä–æ–ª—å —Å–ª—É—á–∞–π–Ω–æ—Å—Ç–∏:
   - `temperature=0.7` - –±–æ–ª–µ–µ –ø—Ä–µ–¥—Å–∫–∞–∑—É–µ–º–æ
   - `temperature=1.0` - —Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–æ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é)
   - `temperature=1.5` - –±–æ–ª–µ–µ –∫—Ä–µ–∞—Ç–∏–≤–Ω–æ
5. **Repetition Penalty** - —É–º–µ–Ω—å—à–µ–Ω–∏–µ –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏–π (penalty=1.2)

## –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ

–ü—Ä–æ–µ–∫—Ç –≤–∫–ª—é—á–∞–µ—Ç 54+ property-based —Ç–µ—Å—Ç–æ–≤ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º Hypothesis:

```bash
# –ó–∞–ø—É—Å–∫ –≤—Å–µ—Ö —Ç–µ—Å—Ç–æ–≤ (–∑–∞–Ω–∏–º–∞–µ—Ç 15-20 –º–∏–Ω—É—Ç)
pytest tests/ -v

# –ó–∞–ø—É—Å–∫ unit —Ç–µ—Å—Ç–æ–≤ (–±—ã—Å—Ç—Ä–æ)
pytest tests/unit/ -v

# –ó–∞–ø—É—Å–∫ property-based —Ç–µ—Å—Ç–æ–≤ (–¥–æ–ª–≥–æ)
pytest tests/property/ -v

# –ó–∞–ø—É—Å–∫ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–æ–Ω–Ω—ã—Ö —Ç–µ—Å—Ç–æ–≤
pytest tests/integration/ -v
```

**–ü—Ä–∏–º–µ—á–∞–Ω–∏–µ**: Property-based —Ç–µ—Å—Ç—ã –∑–∞–ø—É—Å–∫–∞—é—Ç 100+ –∏—Ç–µ—Ä–∞—Ü–∏–π –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Å–≤–æ–π—Å—Ç–≤–∞, –ø–æ—ç—Ç–æ–º—É –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∑–∞–Ω–∏–º–∞–µ—Ç –≤—Ä–µ–º—è.

## –ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è

### –û–±—É—á–µ–Ω–∏–µ –Ω–∞ —Å–≤–æ–∏—Ö –¥–∞–Ω–Ω—ã—Ö

#### Transformer (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è)

1. –°–æ–∑–¥–∞–π—Ç–µ —Ñ–∞–π–ª—ã –≤ `datasets/` —Å –≤–∞—à–∏–º —Ç–µ–∫—Å—Ç–æ–º (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è >10KB)
2. –û–±—ä–µ–¥–∏–Ω–∏—Ç–µ –¥–∞—Ç–∞—Å–µ—Ç—ã (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ):
   ```bash
   python combine_datasets.py
   ```
3. –ó–∞–ø—É—Å—Ç–∏—Ç–µ –æ–±—É—á–µ–Ω–∏–µ:
   ```bash
   python main.py --train --model-type transformer --model-size small --epochs 500 --batch-size 64 --lr 0.0005
   ```
4. –ù–∞—á–Ω–∏—Ç–µ —á–∞—Ç:
   ```bash
   python main.py --model-type transformer --model-size small
   ```

#### LSTM (–¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è)

1. –°–æ–∑–¥–∞–π—Ç–µ —Ñ–∞–π–ª `datasets/my_data.txt` —Å –≤–∞—à–∏–º —Ç–µ–∫—Å—Ç–æ–º
2. –ó–∞–ø—É—Å—Ç–∏—Ç–µ –æ–±—É—á–µ–Ω–∏–µ:
   ```bash
   python main.py --train --model-type lstm --epochs 100 --lr 0.001
   ```
3. –ù–∞—á–Ω–∏—Ç–µ —á–∞—Ç:
   ```bash
   python main.py --model-type lstm
   ```

### –î–æ–æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏

–û–±–µ –º–æ–¥–µ–ª–∏ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—Ç –∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ:

1. –î–æ–±–∞–≤—å—Ç–µ –Ω–æ–≤—ã–µ —Ñ–∞–π–ª—ã –≤ `datasets/`
2. –ó–∞–ø—É—Å—Ç–∏—Ç–µ –æ–±—É—á–µ–Ω–∏–µ —Å–Ω–æ–≤–∞ —Å **–º–µ–Ω—å—à–∏–º learning rate**:

```bash
# Transformer (–∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ –º–µ–Ω—å—à–∏–π lr!)
python main.py --train --model-type transformer --model-size small --epochs 50 --lr 0.00005

# LSTM
python main.py --train --model-type lstm --epochs 30 --lr 0.0001
```

**–í–∞–∂–Ω–æ**: –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –º–µ–Ω—å—à–∏–π learning rate (–≤ 10 —Ä–∞–∑ –º–µ–Ω—å—à–µ) –¥–ª—è –¥–æ–æ–±—É—á–µ–Ω–∏—è, —á—Ç–æ–±—ã –Ω–µ –∑–∞–±—ã—Ç—å –ø—Ä–µ–¥—ã–¥—É—â–∏–µ –∑–Ω–∞–Ω–∏—è!

### –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π

–í—ã –º–æ–∂–µ—Ç–µ –æ–±—É—á–∏—Ç—å –æ–±–µ –º–æ–¥–µ–ª–∏ –∏ —Å—Ä–∞–≤–Ω–∏—Ç—å –∏—Ö:

```bash
# –û–±—É—á–∏—Ç—å LSTM
python main.py --train --model-type lstm --epochs 100

# –û–±—É—á–∏—Ç—å Transformer
python main.py --train --model-type transformer --model-size small --epochs 100

# –ß–∞—Ç —Å LSTM
python main.py --model-type lstm

# –ß–∞—Ç —Å Transformer
python main.py --model-type transformer --model-size small
```

### –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏

#### GPU —É—Å–∫–æ—Ä–µ–Ω–∏–µ

Transformer –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç GPU –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω:

```bash
# Mixed precision training (–≤ 2 —Ä–∞–∑–∞ –±—ã—Å—Ç—Ä–µ–µ –Ω–∞ GPU)
python main.py --train --model-type transformer --model-size medium --epochs 100 --mixed-precision --batch-size 64
```

#### CPU —Ä–µ–∂–∏–º

–î–ª—è CPU —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å small –º–æ–¥–µ–ª—å —Å –º–µ–Ω—å—à–∏–º batch-size:

```bash
python main.py --train --model-type transformer --model-size small --epochs 100 --batch-size 16 --lr 0.0005
```

## –£—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ –Ω–µ–ø–æ–ª–∞–¥–æ–∫

### –ú–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–∞–µ—Ç—Å—è

- –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –≤ `datasets/` –µ—Å—Ç—å —Ñ–∞–π–ª—ã
- –ü—Ä–æ–≤–µ—Ä—å—Ç–µ, —á—Ç–æ —Ñ–∞–π–ª—ã —Å–æ–¥–µ—Ä–∂–∞—Ç –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Ç–µ–∫—Å—Ç–∞:
  - LSTM: –º–∏–Ω–∏–º—É–º 100 —Å–∏–º–≤–æ–ª–æ–≤
  - Transformer: —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è >10KB –¥–ª—è —Ö–æ—Ä–æ—à–µ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞
- –ü–æ–ø—Ä–æ–±—É–π—Ç–µ —É–≤–µ–ª–∏—á–∏—Ç—å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö (500+ –¥–ª—è Transformer)
- –ü—Ä–æ–≤–µ—Ä—å—Ç–µ, —á—Ç–æ learning rate –Ω–µ —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π (0.0005 –¥–ª—è Transformer)

### –ü—Ä–æ–≥—Ä–∞–º–º–∞ –∑–∞–≤–∏—Å–ª–∞ –Ω–∞ "Building BPE vocabulary"

–≠—Ç–æ –Ω–æ—Ä–º–∞–ª—å–Ω–æ! –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ BPE —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ –∑–∞–Ω–∏–º–∞–µ—Ç 2-5 –º–∏–Ω—É—Ç –ø—Ä–∏ –ø–µ—Ä–≤–æ–º –∑–∞–ø—É—Å–∫–µ. –ü–æ–¥–æ–∂–¥–∏—Ç–µ, —Å–∫–æ—Ä–æ –ø–æ—è–≤—è—Ç—Å—è –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä—ã.

### –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ PDF

- –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ PyPDF2: `pip install PyPDF2`
- –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ PDF –Ω–µ –ø–æ–≤—Ä–µ–∂–¥–µ–Ω
- –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å PDF –≤ .txt

### –ú–æ–¥–µ–ª—å –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Å—Ç—Ä–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç

#### LSTM
- –û–±—É—á–∏—Ç–µ –º–æ–¥–µ–ª—å –¥–æ–ª—å—à–µ (100+ —ç–ø–æ—Ö)
- –î–æ–±–∞–≤—å—Ç–µ –±–æ–ª—å—à–µ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
- LSTM –∏–º–µ–µ—Ç –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ - —Ä–∞—Å—Å–º–æ—Ç—Ä–∏—Ç–µ –ø–µ—Ä–µ—Ö–æ–¥ –Ω–∞ Transformer

#### Transformer
- –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –æ–±—É—á–∏–ª–∏ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —ç–ø–æ—Ö (–º–∏–Ω–∏–º—É–º 50-100, —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è 500)
- –î–æ–±–∞–≤—å—Ç–µ –±–æ–ª—å—à–µ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö (>50KB)
- –ü–æ–ø—Ä–æ–±—É–π—Ç–µ —É–≤–µ–ª–∏—á–∏—Ç—å model-size (small ‚Üí medium)
- –£–º–µ–Ω—å—à–∏—Ç–µ learning rate (0.0005 ‚Üí 0.0003)
- –ù–∞—Å—Ç—Ä–æ–π—Ç–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ (temperature, top_k, top_p)

### Out of Memory (OOM)

–ï—Å–ª–∏ –Ω–µ —Ö–≤–∞—Ç–∞–µ—Ç –ø–∞–º—è—Ç–∏ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ Transformer:

1. –£–º–µ–Ω—å—à–∏—Ç–µ batch-size:
   ```bash
   python main.py --train --model-type transformer --batch-size 16
   ```

2. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –º–µ–Ω—å—à—É—é –º–æ–¥–µ–ª—å:
   ```bash
   python main.py --train --model-type transformer --model-size small
   ```

3. –î–ª—è CPU –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ LSTM:
   ```bash
   python main.py --train --model-type lstm --batch-size 16
   ```

### –ú–µ–¥–ª–µ–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ

- –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ GPU –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω (–∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏)
- –í–∫–ª—é—á–∏—Ç–µ mixed precision: `--mixed-precision` (—Ç–æ–ª—å–∫–æ GPU)
- –£–º–µ–Ω—å—à–∏—Ç–µ model-size (medium ‚Üí small)
- –£–º–µ–Ω—å—à–∏—Ç–µ batch-size (64 ‚Üí 32)
- –î–ª—è –±—ã—Å—Ç—Ä—ã—Ö —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ LSTM

### Loss –Ω–µ —É–º–µ–Ω—å—à–∞–µ—Ç—Å—è

- –£–º–µ–Ω—å—à–∏—Ç–µ learning rate (0.0005 ‚Üí 0.0001)
- –£–≤–µ–ª–∏—á—å—Ç–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö
- –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –∫–∞—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö (–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –ª–∏ —Ç–µ–∫—Å—Ç–∞?)
- –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –¥—Ä—É–≥–æ–π —Ä–∞–∑–º–µ—Ä –º–æ–¥–µ–ª–∏

## –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å

### –°—Ä–∞–≤–Ω–µ–Ω–∏–µ LSTM vs Transformer

| –ú–µ—Ç—Ä–∏–∫–∞ | LSTM | Transformer Small | Transformer Medium |
|---------|------|-------------------|-------------------|
| **–°–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è** | –ë—ã—Å—Ç—Ä–æ | –°—Ä–µ–¥–Ω–µ | –ú–µ–¥–ª–µ–Ω–Ω–æ |
| **–ö–∞—á–µ—Å—Ç–≤–æ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏** | –ë–∞–∑–æ–≤–æ–µ | –•–æ—Ä–æ—à–µ–µ | –û—Ç–ª–∏—á–Ω–æ–µ |
| **–ü–∞–º—è—Ç—å –º–æ–¥–µ–ª–∏** | ~10MB | ~50MB | ~200MB |
| **–ö–æ–Ω—Ç–µ–∫—Å—Ç** | –û–≥—Ä–∞–Ω–∏—á–µ–Ω | 256 —Ç–æ–∫–µ–Ω–æ–≤ | 512 —Ç–æ–∫–µ–Ω–æ–≤ |
| **–¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è** | –ü–æ—Å–∏–º–≤–æ–ª—å–Ω–∞—è | BPE (5000) | BPE (5000) |
| **–ü–æ–Ω–∏–º–∞–Ω–∏–µ —Å–ª–æ–≤** | –ù–µ—Ç | –î–∞ | –î–∞ |
| **–ò—Å—Ç–æ—Ä–∏—è –¥–∏–∞–ª–æ–≥–∞** | –ù–µ—Ç | 5 —Å–æ–æ–±—â–µ–Ω–∏–π | 5 —Å–æ–æ–±—â–µ–Ω–∏–π |
| **–ü–∞—Ä–∞–º–µ—Ç—Ä—ã** | ~500K | ~2M | ~15M |

### –í—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è (–ø—Ä–∏–º–µ—Ä–Ω–æ)

**–ù–∞ CPU (Intel i5/i7):**
- LSTM: ~5-10 –º–∏–Ω –Ω–∞ 100 —ç–ø–æ—Ö
- Transformer Small: ~30-60 –º–∏–Ω –Ω–∞ 100 —ç–ø–æ—Ö
- Transformer Medium: ~2-4 —á–∞—Å–∞ –Ω–∞ 100 —ç–ø–æ—Ö

**–ù–∞ GPU (NVIDIA GTX 1060+):**
- LSTM: ~2-5 –º–∏–Ω –Ω–∞ 100 —ç–ø–æ—Ö
- Transformer Small: ~10-20 –º–∏–Ω –Ω–∞ 100 —ç–ø–æ—Ö
- Transformer Medium: ~30-60 –º–∏–Ω –Ω–∞ 100 —ç–ø–æ—Ö

### –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏

- **–î–ª—è –±—ã—Å—Ç—Ä—ã—Ö —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤**: LSTM
- **–î–ª—è –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏**: Transformer Small (‚≠ê —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è)
- **–î–ª—è production**: Transformer Medium —Å GPU
- **–î–ª—è CPU**: Transformer Small —Å batch-size=16
- **–î–ª—è –º–æ—â–Ω–æ–≥–æ GPU**: Transformer Large —Å mixed-precision

## Telegram Bot

–ü—Ä–æ–µ–∫—Ç –≤–∫–ª—é—á–∞–µ—Ç –≥–æ—Ç–æ–≤—ã–π Telegram –±–æ—Ç –¥–ª—è –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è —Å –º–æ–¥–µ–ª—å—é. –°–º. `telegram_bot/README.md` –¥–ª—è –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π.

## –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è

- `README.md` - —ç—Ç–æ—Ç —Ñ–∞–π–ª
- `MIGRATION_GUIDE.md` - —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ –ø–æ –º–∏–≥—Ä–∞—Ü–∏–∏ —Å LSTM –Ω–∞ Transformer
- `IMPLEMENTATION_SUMMARY.md` - –¥–µ—Ç–∞–ª–∏ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏
- `START_AI_MODEL.md` - –±—ã—Å—Ç—Ä—ã–π —Å—Ç–∞—Ä—Ç

## –õ–∏—Ü–µ–Ω–∑–∏—è

–≠—Ç–æ—Ç –ø—Ä–æ–µ–∫—Ç —Å–æ–∑–¥–∞–Ω –¥–ª—è –æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω—ã—Ö —Ü–µ–ª–µ–π.

## –ê–≤—Ç–æ—Ä—ã

- **–°–æ–∑–¥–∞—Ç–µ–ª—å**: Jamsaide (–ê—Ä—Ç—É—Ä)
- **–ü–æ–º–æ—â–Ω–∏–∫–∏**: BeeBoo (–ù–∏–∫–∏—Ç–∞), Araxium (–õ–µ–≤)

---

**–í–µ—Ä—Å–∏—è**: 2.0 (Transformer Upgrade)  
**–î–∞—Ç–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è**: –§–µ–≤—Ä–∞–ª—å 2026  
**GitHub**: https://github.com/Artur566754/BeeJa-AI
